# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11FOX4G6GpqE2cg11H6wc_bkKMgXG-b98
"""

!pip install SentencePiece

!wget https://ciir.cs.umass.edu/downloads/LaMP/LaMP_4/train/train_questions.json news_train_data.json

from transformers import MarianMTModel, MarianTokenizer
import torch
class TranslateMain:
    def __init__(self, model_name_fwd='Helsinki-NLP/opus-mt-en-es', model_name_reverse='Helsinki-NLP/opus-mt-es-en'):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.model_fwd = MarianMTModel.from_pretrained(model_name_fwd)
        self.tokenizer_fwd = MarianTokenizer.from_pretrained(model_name_fwd)
        self.model_reverse = MarianMTModel.from_pretrained(model_name_reverse)
        self.tokenizer_reverse = MarianTokenizer.from_pretrained(model_name_reverse)

    def translate_pass(self, texts, pass_fwd_or_rev):
        if pass_fwd_or_rev == 'fwd':
            tokenizer = self.tokenizer_fwd
            model = self.model_fwd.to(self.device)
        else:
            tokenizer = self.tokenizer_reverse
            model = self.model_reverse.to(self.device)
        # Tokenize input texts
        input_ids = tokenizer.batch_encode_plus(texts, return_tensors='pt', padding=True, truncation=True)['input_ids']

        input_ids = input_ids.to(self.device)
        # Perform translation
        output_ids = model.generate(input_ids)

        # output_ids = output_ids.to('cpu')

        # Decode the translated texts
        translated_texts = tokenizer.batch_decode(output_ids, skip_special_tokens=True)

        input_ids.to('cpu')
        return translated_texts

    def rephrase_using_translation(self, texts):
        fwd_pass_text = self.translate_pass(texts, pass_fwd_or_rev='fwd')
        print(f'Intermediate translation: {fwd_pass_text}')
        rev_pass_text = self.translate_pass(fwd_pass_text, pass_fwd_or_rev='rev')
        return fwd_pass_text, rev_pass_text



# Opening JSON file

# # Example usage
# translator = TranslateMain()

# texts_to_translate = ["Hello, how are you?", "@PhotOle, we also walked down to the blockade tonight for dinner and saw it, but were unaware of the situation."]
# rephrased_texts = translator.rephrase_using_translation(texts_to_translate)

# for original_text, rephrased_text in zip(texts_to_translate, rephrased_texts):
#     print(f"Original: {original_text}")
#     print(f"Rephrased: {rephrased_text}")
#     print()

just_150_data = data[0:150]

file_path = "just_150_data.json"

# Save the dictionary to a JSON file
with open(file_path, "w") as json_file:
    json.dump(just_150_data, json_file)

print(f"The dictionary has been saved to {file_path}")

f = open('/content/just_150_data.json')

# returns JSON object as
# a dictionary
data = json.load(f)

data[0]

def get_remaining_string(original_string, substring):
    index = original_string.find(substring)

    if index != -1:
        # If the substring is found, return the part of the string after the substring
        remaining_string = original_string[index + len(substring):]
        return remaining_string
    else:
        # If the substring is not found, return the original string
        return original_string

substring = "Generate a headline for the following article:"
only_the_tweet = []
for i in data:
    only_the_tweet.append(get_remaining_string(i['input'], substring))



from tqdm import tqdm
import pandas as pd
# len(original_text)
def split_list(input_list, chunk_size):
    return [input_list[i:i + chunk_size] for i in range(0, len(input_list), chunk_size)]

# Example usage
original_list = only_the_tweet
chunk_size = 50

result = split_list(original_list, chunk_size)
checkpoint = -1
# Print the result
list_of_df = []
translator = TranslateMain()
for i, chunk in tqdm(enumerate(result), total=len(result), desc="Processing transaltion"):
    if i>checkpoint:
      torch.cuda.empty_cache()
      original_text = chunk
      rephrased_texts = translator.rephrase_using_translation(original_text)
      df = pd.DataFrame({"original text": original_text, 'fwd_translated': rephrased_texts[0], 'rev_translated': rephrased_texts[1]})
      torch.cuda.empty_cache()
      list_of_df.append(df)

result = pd.concat(list_of_df, axis=0, ignore_index=True)
result.to_csv("news_150_en_es_en.csv")

